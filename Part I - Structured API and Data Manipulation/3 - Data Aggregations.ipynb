{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Data Aggregations\n",
    "\n",
    "Jia Geng | gjia0214@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='directory'></a>\n",
    "\n",
    "## Directory\n",
    "\n",
    "- [Data Source](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/)\n",
    "- [1. DataFrame Level Aggregation](#sec1)\n",
    "- [2. GroupBy and Aggregate](#sec2)\n",
    "- [3. Window Function](#sec3)\n",
    "- [4. GroupSets, Rollup, and Cube](#sec4)\n",
    "- [5. Pivot](#sec5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\r\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~19.10-b09)\r\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "# check java version \n",
    "# use sudo update-alternatives --config java to switch java version if needed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Learning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb3f801df90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark Learning').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = '/home/jgeng/Documents/Git/SparkLearning/book_data/retail-data/all/online-retail-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example)\n",
    "df.printSchema()\n",
    "df.show(3)\n",
    "df.cache()  # cache is lazy operation, it does not cache data until use it\n",
    "df.count()  # since count is an action on all data, call this will cache all data on memory!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "Aggregation is to group the rows by a key and grouping function. In spark, the groupby operation will return a `RelationalGroupedDataset` object.\n",
    "\n",
    "Grouping types in spark include:\n",
    "- Dataframe level aggregation.\n",
    "- **group by**: Aggregate using one or more keys and one or more grouping functions\n",
    "- **window**: Aggregate using one or more keys and one or more grouping functions. Functions are related to the current row.\n",
    "- **group set**: Aggregate at multiple different levels\n",
    "    - **roll up**: one or more keys and one or more values, summarized hierarchically\n",
    "    - **cube**: one or more keys and one or more values, summarized across all combinations of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DataFrame Level Aggregation <a id='sec1'></a>\n",
    "\n",
    "Common aggregation functions on dataframes are under `pyspark.sql.functions`. Work on columns.\n",
    "- `count()`: `df.count()` is action.\n",
    "- `countDistinct()`: can be slow when data is large\n",
    "- `approx_count_distinct(col_name, prec)`: faster option, take a precision param\n",
    "- `first()`, `last()`: get first/last value of a column\n",
    "- `min()`, `max()`, `sum()`, `sumDistinct()`, `avg()`: work as it means\n",
    "- `var_pop()`, `var_sample()`, `stddev_pop()`, `stddev_sample()`: work as it means\n",
    "- `skewness()`, `kurtosis()`\n",
    "    - skewness: Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n",
    "        - normal dist. skewness = 0 (symmetry, left/right tails are same)\n",
    "        - positive skewness: right skew - right tail is longer\n",
    "        - negative skewness: left skew - left tail is longer\n",
    "    - kurtosis: Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.\n",
    "        - normal dist. kurtosis = 0\n",
    "        - positive kurtosis: heavy tailed\n",
    "        - negative kurtosis: light tailed\n",
    "    \n",
    "- `corr()`, `covar_pop()`, `covar_sample()`\n",
    "\n",
    "Spark also support aggregate column values into an array using `collect_set()` or `collect_list()` fucntion.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|         4070|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|         4079|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|DistinctCount|\n",
      "+-------------+\n",
      "|        45280|\n",
      "+-------------+\n",
      "\n",
      "+------+\n",
      "|Approx|\n",
      "+------+\n",
      "| 45378|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|Approx|\n",
      "+------+\n",
      "| 45314|\n",
      "+------+\n",
      "\n",
      "+---------+--------+\n",
      "|StockCode|Quantity|\n",
      "+---------+--------+\n",
      "|    21485|       6|\n",
      "|    84347|       3|\n",
      "|    22454|       2|\n",
      "+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45280"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, approx_count_distinct, col, struct, array\n",
    "# count, distince count, count coloumn \n",
    "df.select(countDistinct(col('StockCode')).alias('DistinctCount')).show()\n",
    "\n",
    "# work faster when data is very large\n",
    "df.select(approx_count_distinct(col('StockCode'), 0.01).alias('DistinctCount')).show()\n",
    "\n",
    "# can count distinct multiple columns\n",
    "df.select(countDistinct(col('StockCode'), col('Quantity')).alias('DistinctCount')).show()  \n",
    "\n",
    "# this would work on multiple columns but slower\n",
    "df.select(approx_count_distinct(struct(col('StockCode'), col('Quantity')), 0.01).alias('Approx')).show()\n",
    "\n",
    "# this would also work on multiple columns but slower\n",
    "df.select(approx_count_distinct(array(col('StockCode'), col('Quantity')), 0.01).alias('Approx')).show()\n",
    "\n",
    "# refresh the use of distinct() to show all distinct rows\n",
    "df.select(col('StockCode'), col('Quantity')).distinct().show(3)\n",
    "df.select(col('StockCode'), col('Quantity')).distinct().count()  # same results as countDistinct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "|   min|  max|first|last|    sum|             avg|               var|           skewness|          kurtosis|\n",
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "|-80995|80995|    6|   3|5176450|9.55224954743324|47559.303646609165|-0.2640755761052369|119768.05495536828|\n",
      "+------+-----+-----+----+-------+----------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, first, last, sum, avg, var_pop, skewness, kurtosis\n",
    "\n",
    "# some column based stats\n",
    "min_quantity = min(df.Quantity)\n",
    "max_quantity = max(df.Quantity)\n",
    "first_quantity = first(df.Quantity)\n",
    "last_quantity = last(df.Quantity)\n",
    "sum_quantity = sum(df.Quantity)\n",
    "avg_quantity = avg(df.Quantity)\n",
    "var_quantity = var_pop(df.Quantity)\n",
    "skewness_quantity = skewness(df.Quantity)\n",
    "kurtosis_quantity = kurtosis(df.Quantity)\n",
    "\n",
    "df.select(min_quantity.alias('min'), max_quantity.alias('max'), \n",
    "          first_quantity.alias('first'), last_quantity.alias('last'),\n",
    "          sum_quantity.alias('sum'), avg_quantity.alias('avg'),\n",
    "          var_quantity.alias('var'), skewness_quantity.alias('skewness'),\n",
    "          kurtosis_quantity.alias('kurtosis')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|         Correlation|         Covariance|\n",
      "+--------------------+-------------------+\n",
      "|-0.00123492454487...|-26.058713170967746|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop\n",
    "\n",
    "# correlation between two columns\n",
    "cor_qp = corr(df.Quantity, df.UnitPrice)\n",
    "\n",
    "# correlation is covariance normalized by variance (pop/sample)\n",
    "covar_qp = covar_pop(df.Quantity, df.UnitPrice)\n",
    "\n",
    "# print it out\n",
    "df.select(cor_qp.alias('Correlation'), covar_qp.alias('Covariance')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+\n",
      "|collect_set(Quantity)|collect_list(Quantity)|\n",
      "+---------------------+----------------------+\n",
      "| [-42, 306, 256, 1...|  [6, 6, 8, 6, 6, 2...|\n",
      "+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "agged_df = df.agg(collect_set(col('Quantity')), collect_list(col('Quantity')))\n",
    "agged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GroupBy and Aggregate <a id='sec2'></a>\n",
    "\n",
    "More common task is to perform calculation based on the groups in the data. This is usually a two stage process:\n",
    "- group by some keys: `.groupBy(col_names, ...)`, support multiple comlumns\n",
    "- aggregate by some function `.agg(func(col), ...)`, this can take multiple functions!\n",
    "\n",
    "`.groupBy()` return a `GroupedData` object which supports `.agg()` that can take aggregation functions on the data groups.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----+------------------+\n",
      "|StockCode|       Country|Count|               Avg|\n",
      "+---------+--------------+-----+------------------+\n",
      "|    22154|United Kingdom|  170|0.5414117647058824|\n",
      "|    22478|United Kingdom|  133|1.8110526315789475|\n",
      "|    22844|United Kingdom|  402|10.921791044776118|\n",
      "+---------+--------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------------+-----+------------------+\n",
      "|StockCode|       Country|Count|               Avg|\n",
      "+---------+--------------+-----+------------------+\n",
      "|    22154|United Kingdom|  170|0.5414117647058824|\n",
      "|    22478|United Kingdom|  133|1.8110526315789475|\n",
      "|    22844|United Kingdom|  402|10.921791044776118|\n",
      "+---------+--------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, expr, col\n",
    "\n",
    "# group by can work with multiple columns\n",
    "df.groupBy('StockCode', 'Country').agg(count(col('StockCode')).alias('Count'), \n",
    "                                       avg(col('UnitPrice')).alias('Avg')).show(3)\n",
    "# can use expr for full string implementation\n",
    "df.groupBy('StockCode', 'Country').agg(expr('count(StockCode)').alias('Count'), \n",
    "                                       expr('avg(UnitPrice)').alias('Avg')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Window Functions <a id='sec3'></a>\n",
    "\n",
    "A window is a specification of which rows should be used for the computation (aggregations).\n",
    "- for groupBy, each row can only go into one group\n",
    "- **for window, a row can go into multiple groups. e.g. rolling average**\n",
    "\n",
    "The pipeline for applying window function is:\n",
    "- define a window, use `Window` object under `pyspark.sql.window`\n",
    "    - use `.partitionBy(col_names, ...)` to define the partitions\n",
    "    - use `.orderBy()` to sort values within each partition\n",
    "    - use `.rowsBeteen()` to define the criteria to generate the window. E.g. `.rowsBetween(Window.unboundedPreceding, Window.currentRow)` means\n",
    "        - a window consist of all previous row -> current row (**within the same partition**)\n",
    "    - above returns a `windowSpec` object\n",
    "\n",
    "Some aggregation functions can be apply on the windowSpec object, for example:\n",
    "- `mean(col_name).over(windowSpect)`: simply use `.over()`\n",
    "- the function should have a string column name input instead of df.colname\n",
    "\n",
    "There are also window functions such as `rank`, `dense_rank`:\n",
    "- `rank().over(windowSpec)`\n",
    "- **since this is a window function, the rank is per partition not the global rank**\n",
    "\n",
    "A common window function pipeline is:\n",
    "- create the `windowSpec`: `.partition(col_names, ...) -> .orderBy() -> `.rowsBetween(start, end)``\n",
    "- apply functions on window to get the column object\n",
    "- select the dataframe using the column object (it is a good pratice to sort the dataframe using the partition criteria for display the data)\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8| 2010-12-01|     2.75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df.show(1)\n",
    "df.printSchema()\n",
    "\n",
    "# convert the InvoiceDate from datetime to date\n",
    "# to_date(col, format), format must be specified or it will not be able to recognize\n",
    "dfWithDate = df.withColumn('InvoiceDate', to_date(col('InvoiceDate'), 'MM/d/yyyy H:mm'))\n",
    "dfWithDate.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "|CustomerId|InvoiceDate|Quantity|Rolling Max Quantity|Rank|Dense Rank|\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "|      null| 2010-12-01|     -10|                 -10|   1|         1|\n",
      "|      null| 2010-12-01|       1|                   1|   2|         2|\n",
      "|      null| 2010-12-01|       1|                   1|   2|         2|\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "|CustomerId|InvoiceDate|Quantity|Rolling Max Quantity|Rank|Dense Rank|\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "|     12346| 2011-01-18|  -74215|              -74215|   1|         1|\n",
      "|     12346| 2011-01-18|   74215|               74215|   2|         2|\n",
      "|     12347| 2010-12-07|       3|                   3|   1|         1|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       4|                   4|   2|         2|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|       6|                   6|   9|         3|\n",
      "|     12347| 2010-12-07|      12|                  12|  16|         4|\n",
      "|     12347| 2010-12-07|      12|                  12|  16|         4|\n",
      "|     12347| 2010-12-07|      12|                  12|  16|         4|\n",
      "+----------+-----------+--------+--------------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max, dense_rank, rank\n",
    "\n",
    "# Step 1 - create window spec\n",
    "# pipeline to define a window spec\n",
    "# define partition -> order -> define range criteria\n",
    "windowSpecPartitioned = Window.partitionBy('CustomerId', 'InvoiceDate')\n",
    "windowSpecOrdered = windowSpecPartitioned.orderBy('Quantity')\n",
    "windowSpec = windowSpecOrdered.rowsBetween(Window.unboundedPreceding, Window.currentRow)  # start, end\n",
    "\n",
    "# Step 2 - apply function over the windowSpec to get the columns\n",
    "# These are all transformations and won't execute right away\n",
    "# When we call actions and execute these queries, it will match column names with the dataframe\n",
    "rollingMaxQuantity = max(col('Quantity')).over(windowSpec)\n",
    "quanRank = rank().over(windowSpec)\n",
    "quanDenseRank = dense_rank().over(windowSpec)\n",
    "\n",
    "# Step 3 - select columns \n",
    "# make sure to remove the nulls\n",
    "dfWithDate.orderBy('CustomerId').select('CustomerId', 'InvoiceDate', 'Quantity',\n",
    "                                rollingMaxQuantity.alias('Rolling Max Quantity'),\n",
    "                                quanRank.alias('Rank'), quanDenseRank.alias('Dense Rank')).show(3)\n",
    "\n",
    "# Step 3 - select columns \n",
    "# make sure to remove the nulls\n",
    "dfWithDate.where('CustomerId is not null').orderBy('CustomerId').select('CustomerId', 'InvoiceDate', 'Quantity',\n",
    "                                                                rollingMaxQuantity.alias('Rolling Max Quantity'),\n",
    "                                                                quanRank.alias('Rank'), quanDenseRank.alias('Dense Rank')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Group Sets <a id='sec4'></a>\n",
    "\n",
    "Three types of group sets aggregation given (col1, col2, col3):\n",
    "- group by (col1, col2, col3)\n",
    "- roll up - hierarchically groups: (all, all, all), (col1, all, all), (col1, col2, all), (col1, col2, col3)\n",
    "- cube - all combination groups: (all, all, all), (col1, all, all), (col2, all, all), (col3, all, all), (col1, all, col3), (all, col2, col3), (col1, col2, all), (col1, col2, col3)\n",
    "\n",
    "Aggregation on multiple groups can be easily achieved by `df.groupBy(col1, col2, col3)`\n",
    "\n",
    "**Roll Up (col1, col2, col3)** -> rollup col1 on the rest of the columns, gives us 4 levels:\n",
    "- grand total\n",
    "- sub total of each (col1) group\n",
    "- sub total of each (col1, col2) group\n",
    "- subtotal of each (col1, col2, col3) group\n",
    "\n",
    "**Cube (col1, col2, col3)** -> all combination aggregation, gives us 8 level (below is not the actual order of levels, check the code example):\n",
    "- grand total\n",
    "- sub total of each (col1) group\n",
    "- sub total of each (col2) group\n",
    "- sub total of each (col3) group\n",
    "- sub total of each (col1, col2) group\n",
    "- sub total of each (col1, col3) group\n",
    "- sub total of each (col2, col3) group\n",
    "- sub total of each (col1, col2, col3) group\n",
    "\n",
    "After the group set operations. We need to query the aggregated information. When doing the `.agg()`, we can introduce a `grouping_id()` function to introduce a column that indicate the level of aggregation.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerID|StockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "+----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# groupBy, each group is defined by the (customerID, stockCode)\n",
    "dfWithDate.groupBy('CustomerID', 'StockCode').agg(sum('Quantity')).orderBy(desc('CustomerID'), desc('StockCode')).show(3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithDate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406829"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithDateNoNull = dfWithDate.na.drop()\n",
    "dfWithDateNoNull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+------+\n",
      "|CustomerID|StockCode|InvoiceDate| count|\n",
      "+----------+---------+-----------+------+\n",
      "|      null|     null|       null|406829|\n",
      "|     12346|     null|       null|     2|\n",
      "|     12346|    23166| 2011-01-18|     2|\n",
      "+----------+---------+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "1 4373 271988\n"
     ]
    }
   ],
   "source": [
    "# rollup \n",
    "# after roll up, we can also use agg when needed\n",
    "rolledDF = dfWithDateNoNull.rollup('CustomerID', 'StockCode', 'InvoiceDate').count().orderBy('CustomerID', 'StockCode')\n",
    "\n",
    "# (null, null) is the sum over all rows\n",
    "# (12345, null) is the sum over customerID = 12345 and all stockcode \n",
    "rolledDF.show(3)\n",
    "\n",
    "# rollup on the left!!!!\n",
    "# rollup col1 on col2, col3\n",
    "# rollup col2 on col3\n",
    "col1_count = rolledDF.where('CustomerID is null').count()\n",
    "col2_count = rolledDF.where('StockCode is null').count()\n",
    "col3_count = rolledDF.where('InvoiceDate is null').count()\n",
    "\n",
    "# number of null means number of totals\n",
    "# for col1, only one grand total\n",
    "# for col2, 4372 sub-totals of each (col1, col2) pairs + 1 grand total\n",
    "# for col3, 271988 include all above + subtotals of each (col1, col2, col3) triplet\n",
    "print(col1_count, col2_count, col3_count)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+------------------+\n",
      "|CustomerID|StockCode|InvoiceDate|     avg(Quantity)|\n",
      "+----------+---------+-----------+------------------+\n",
      "|      null|    21390| 2010-12-02|              24.0|\n",
      "|      null|    21025|       null|              6.25|\n",
      "|      null|    21563|       null|10.134328358208956|\n",
      "|      null|    21915|       null| 36.36869565217391|\n",
      "|      null|    22437| 2010-12-02|              20.0|\n",
      "+----------+---------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913656"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "# cube\n",
    "dfWithDateNoNull.cube('CustomerID', 'StockCode', 'InvoiceDate').agg(avg('Quantity')).orderBy('CustomerID').show(5)\n",
    "\n",
    "# more levels\n",
    "dfWithDateNoNull.cube('CustomerID', 'StockCode', 'InvoiceDate').agg(avg('Quantity')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|     avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|      null|    22438|       null|    5|12.162162162162161|          900|\n",
      "|      null|   85169B|       null|    5|              8.04|          201|\n",
      "|      null|    21363|       null|    5|1.9306930693069306|          195|\n",
      "|      null|    22117|       null|    5|17.916666666666668|         1290|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|    avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|      null|     null|       null|    7|12.06130339774205|      4906888|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "\n",
      "Level 0\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|     18074|    22189| 2010-12-01|    0|          4.0|            4|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 1\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|     16098|    21832|       null|    1|         12.0|           12|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 2\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|     avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|     15260|     null| 2010-12-02|    2|14.941176470588236|          254|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 3\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|    avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|     15100|     null|       null|    3|9.666666666666666|           58|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 4\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|    avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|      null|    22603| 2010-12-01|    4|4.333333333333333|           13|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 5\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "|      null|    21739|       null|    5|         2.86|          143|\n",
      "+----------+---------+-----------+-----+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 6\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|     avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "|      null|     null| 2011-03-31|    6|12.923698384201078|        14397|\n",
      "+----------+---------+-----------+-----+------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Level 7\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|CustomerID|StockCode|InvoiceDate|level|    avg(Quantity)|sum(Quantity)|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "|      null|     null|       null|    7|12.06130339774205|      4906888|\n",
      "+----------+---------+-----------+-----+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import grouping_id\n",
    "\n",
    "# higher id means higher level of aggregation\n",
    "# the highest id means the grand total\n",
    "dfCubed = dfWithDateNoNull.cube('CustomerID', 'StockCode', 'InvoiceDate').agg(grouping_id().alias('level'), avg('Quantity'), sum('Quantity'))  \n",
    "dfCubed.orderBy('CustomerID').show(4)\n",
    "\n",
    "# to get the grand total, we just need to query on the record with the highest level\n",
    "# cube will produce 8 levels, hence the highest level is 7\n",
    "dfCubed.where('level == 7').show()  # bingo!\n",
    "\n",
    "# lets check each level\n",
    "for i in range(8):\n",
    "    query = 'level == {}'.format(i)\n",
    "    print('Level {}'.format(i))\n",
    "    dfCubed.where(query).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pivot <a id='sec5'></a>\n",
    "\n",
    "Pivot brings the aggregated values of some feature to columns and display them in a easy-to-equery way.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "|  X|  S|  1|  3|\n",
      "|  X|  P|  1|  6|\n",
      "|  Y|  P|  1|  8|\n",
      "|  Y|  P|  1| 10|\n",
      "|  Y|  S|  1|  0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For example, if we have a table\n",
    "data = [('X', 'S', 1, 3), ('X', 'P', 1, 6), ('Y', 'P', 1, 8), ('Y', 'P', 1, 10), ('Y', 'S', 1, 0)]\n",
    "col_names = ['A', 'B', 'C', 'D']\n",
    "dfExample = spark.createDataFrame(data, col_names)\n",
    "dfExample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|  A|  B|sum(C)|\n",
      "+---+---+------+\n",
      "|  X|  S|     1|\n",
      "|  Y|  P|     2|\n",
      "|  Y|  S|     1|\n",
      "|  X|  P|     1|\n",
      "+---+---+------+\n",
      "\n",
      "+---+---+------+\n",
      "|  A|  B|sum(C)|\n",
      "+---+---+------+\n",
      "|  X|  P|     1|\n",
      "+---+---+------+\n",
      "\n",
      "+---+--------+--------+--------+--------+\n",
      "|  A|P_sum(C)|P_sum(D)|S_sum(C)|S_sum(D)|\n",
      "+---+--------+--------+--------+--------+\n",
      "|  Y|       2|      18|       1|       0|\n",
      "|  X|       1|       6|       1|       3|\n",
      "+---+--------+--------+--------+--------+\n",
      "\n",
      "+--------+\n",
      "|P_sum(C)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if our task demands frequent query things like sum(C) for all A == X , B == D.\n",
    "# we could simple create a aggregated table for the queries\n",
    "dfExampleAgg = dfExample.groupBy('A', 'B').agg(sum(col('C')))\n",
    "dfExampleAgg.show()\n",
    "\n",
    "# queries would be\n",
    "dfExampleAgg.where((col('A') == 'X') & (col('B') == 'P')).show()\n",
    "\n",
    "# anther way is to pivot - now the original B col data 'S', 'D' are columns\n",
    "# since B col have two possible values S, D and we have two numeric column C, D\n",
    "# we aggregate using sum\n",
    "# the pivoting will create 2x2 = 4 pivoted columns\n",
    "pivotExample = dfExample.groupBy('A').pivot('B').sum()\n",
    "pivotExample.show()\n",
    "\n",
    "# queries would be\n",
    "pivotExample.where(col('A') == 'X').select('P_sum(C)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8| 2010-12-01|     2.75|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- Australia_sum(Quantity): long (nullable = true)\n",
      " |-- Australia_sum(UnitPrice): double (nullable = true)\n",
      " |-- Australia_sum(CustomerID): long (nullable = true)\n",
      " |-- Austria_sum(Quantity): long (nullable = true)\n",
      " |-- Austria_sum(UnitPrice): double (nullable = true)\n",
      " |-- Austria_sum(CustomerID): long (nullable = true)\n",
      " |-- Bahrain_sum(Quantity): long (nullable = true)\n",
      " |-- Bahrain_sum(UnitPrice): double (nullable = true)\n",
      " |-- Bahrain_sum(CustomerID): long (nullable = true)\n",
      " |-- Belgium_sum(Quantity): long (nullable = true)\n",
      " |-- Belgium_sum(UnitPrice): double (nullable = true)\n",
      " |-- Belgium_sum(CustomerID): long (nullable = true)\n",
      " |-- Brazil_sum(Quantity): long (nullable = true)\n",
      " |-- Brazil_sum(UnitPrice): double (nullable = true)\n",
      " |-- Brazil_sum(CustomerID): long (nullable = true)\n",
      " |-- Canada_sum(Quantity): long (nullable = true)\n",
      " |-- Canada_sum(UnitPrice): double (nullable = true)\n",
      " |-- Canada_sum(CustomerID): long (nullable = true)\n",
      " |-- Channel Islands_sum(Quantity): long (nullable = true)\n",
      " |-- Channel Islands_sum(UnitPrice): double (nullable = true)\n",
      " |-- Channel Islands_sum(CustomerID): long (nullable = true)\n",
      " |-- Cyprus_sum(Quantity): long (nullable = true)\n",
      " |-- Cyprus_sum(UnitPrice): double (nullable = true)\n",
      " |-- Cyprus_sum(CustomerID): long (nullable = true)\n",
      " |-- Czech Republic_sum(Quantity): long (nullable = true)\n",
      " |-- Czech Republic_sum(UnitPrice): double (nullable = true)\n",
      " |-- Czech Republic_sum(CustomerID): long (nullable = true)\n",
      " |-- Denmark_sum(Quantity): long (nullable = true)\n",
      " |-- Denmark_sum(UnitPrice): double (nullable = true)\n",
      " |-- Denmark_sum(CustomerID): long (nullable = true)\n",
      " |-- EIRE_sum(Quantity): long (nullable = true)\n",
      " |-- EIRE_sum(UnitPrice): double (nullable = true)\n",
      " |-- EIRE_sum(CustomerID): long (nullable = true)\n",
      " |-- European Community_sum(Quantity): long (nullable = true)\n",
      " |-- European Community_sum(UnitPrice): double (nullable = true)\n",
      " |-- European Community_sum(CustomerID): long (nullable = true)\n",
      " |-- Finland_sum(Quantity): long (nullable = true)\n",
      " |-- Finland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Finland_sum(CustomerID): long (nullable = true)\n",
      " |-- France_sum(Quantity): long (nullable = true)\n",
      " |-- France_sum(UnitPrice): double (nullable = true)\n",
      " |-- France_sum(CustomerID): long (nullable = true)\n",
      " |-- Germany_sum(Quantity): long (nullable = true)\n",
      " |-- Germany_sum(UnitPrice): double (nullable = true)\n",
      " |-- Germany_sum(CustomerID): long (nullable = true)\n",
      " |-- Greece_sum(Quantity): long (nullable = true)\n",
      " |-- Greece_sum(UnitPrice): double (nullable = true)\n",
      " |-- Greece_sum(CustomerID): long (nullable = true)\n",
      " |-- Hong Kong_sum(Quantity): long (nullable = true)\n",
      " |-- Hong Kong_sum(UnitPrice): double (nullable = true)\n",
      " |-- Hong Kong_sum(CustomerID): long (nullable = true)\n",
      " |-- Iceland_sum(Quantity): long (nullable = true)\n",
      " |-- Iceland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Iceland_sum(CustomerID): long (nullable = true)\n",
      " |-- Israel_sum(Quantity): long (nullable = true)\n",
      " |-- Israel_sum(UnitPrice): double (nullable = true)\n",
      " |-- Israel_sum(CustomerID): long (nullable = true)\n",
      " |-- Italy_sum(Quantity): long (nullable = true)\n",
      " |-- Italy_sum(UnitPrice): double (nullable = true)\n",
      " |-- Italy_sum(CustomerID): long (nullable = true)\n",
      " |-- Japan_sum(Quantity): long (nullable = true)\n",
      " |-- Japan_sum(UnitPrice): double (nullable = true)\n",
      " |-- Japan_sum(CustomerID): long (nullable = true)\n",
      " |-- Lebanon_sum(Quantity): long (nullable = true)\n",
      " |-- Lebanon_sum(UnitPrice): double (nullable = true)\n",
      " |-- Lebanon_sum(CustomerID): long (nullable = true)\n",
      " |-- Lithuania_sum(Quantity): long (nullable = true)\n",
      " |-- Lithuania_sum(UnitPrice): double (nullable = true)\n",
      " |-- Lithuania_sum(CustomerID): long (nullable = true)\n",
      " |-- Malta_sum(Quantity): long (nullable = true)\n",
      " |-- Malta_sum(UnitPrice): double (nullable = true)\n",
      " |-- Malta_sum(CustomerID): long (nullable = true)\n",
      " |-- Netherlands_sum(Quantity): long (nullable = true)\n",
      " |-- Netherlands_sum(UnitPrice): double (nullable = true)\n",
      " |-- Netherlands_sum(CustomerID): long (nullable = true)\n",
      " |-- Norway_sum(Quantity): long (nullable = true)\n",
      " |-- Norway_sum(UnitPrice): double (nullable = true)\n",
      " |-- Norway_sum(CustomerID): long (nullable = true)\n",
      " |-- Poland_sum(Quantity): long (nullable = true)\n",
      " |-- Poland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Poland_sum(CustomerID): long (nullable = true)\n",
      " |-- Portugal_sum(Quantity): long (nullable = true)\n",
      " |-- Portugal_sum(UnitPrice): double (nullable = true)\n",
      " |-- Portugal_sum(CustomerID): long (nullable = true)\n",
      " |-- RSA_sum(Quantity): long (nullable = true)\n",
      " |-- RSA_sum(UnitPrice): double (nullable = true)\n",
      " |-- RSA_sum(CustomerID): long (nullable = true)\n",
      " |-- Saudi Arabia_sum(Quantity): long (nullable = true)\n",
      " |-- Saudi Arabia_sum(UnitPrice): double (nullable = true)\n",
      " |-- Saudi Arabia_sum(CustomerID): long (nullable = true)\n",
      " |-- Singapore_sum(Quantity): long (nullable = true)\n",
      " |-- Singapore_sum(UnitPrice): double (nullable = true)\n",
      " |-- Singapore_sum(CustomerID): long (nullable = true)\n",
      " |-- Spain_sum(Quantity): long (nullable = true)\n",
      " |-- Spain_sum(UnitPrice): double (nullable = true)\n",
      " |-- Spain_sum(CustomerID): long (nullable = true)\n",
      " |-- Sweden_sum(Quantity): long (nullable = true)\n",
      " |-- Sweden_sum(UnitPrice): double (nullable = true)\n",
      " |-- Sweden_sum(CustomerID): long (nullable = true)\n",
      " |-- Switzerland_sum(Quantity): long (nullable = true)\n",
      " |-- Switzerland_sum(UnitPrice): double (nullable = true)\n",
      " |-- Switzerland_sum(CustomerID): long (nullable = true)\n",
      " |-- USA_sum(Quantity): long (nullable = true)\n",
      " |-- USA_sum(UnitPrice): double (nullable = true)\n",
      " |-- USA_sum(CustomerID): long (nullable = true)\n",
      " |-- United Arab Emirates_sum(Quantity): long (nullable = true)\n",
      " |-- United Arab Emirates_sum(UnitPrice): double (nullable = true)\n",
      " |-- United Arab Emirates_sum(CustomerID): long (nullable = true)\n",
      " |-- United Kingdom_sum(Quantity): long (nullable = true)\n",
      " |-- United Kingdom_sum(UnitPrice): double (nullable = true)\n",
      " |-- United Kingdom_sum(CustomerID): long (nullable = true)\n",
      " |-- Unspecified_sum(Quantity): long (nullable = true)\n",
      " |-- Unspecified_sum(UnitPrice): double (nullable = true)\n",
      " |-- Unspecified_sum(CustomerID): long (nullable = true)\n",
      "\n",
      "+-----------+-----------------------+------------------------+-------------------------+---------------------+----------------------+-----------------------+---------------------+----------------------+-----------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+--------------------+---------------------+----------------------+-----------------------------+------------------------------+-------------------------------+--------------------+---------------------+----------------------+----------------------------+-----------------------------+------------------------------+---------------------+----------------------+-----------------------+------------------+-------------------+--------------------+--------------------------------+---------------------------------+----------------------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+-------------------+--------------------+---------------------+-------------------+--------------------+---------------------+---------------------+----------------------+-----------------------+-----------------------+------------------------+-------------------------+-------------------+--------------------+---------------------+-------------------------+--------------------------+---------------------------+--------------------+---------------------+----------------------+--------------------+---------------------+----------------------+----------------------+-----------------------+------------------------+-----------------+------------------+-------------------+--------------------------+---------------------------+----------------------------+-----------------------+------------------------+-------------------------+-------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-------------------------+--------------------------+---------------------------+-----------------+------------------+-------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------+-----------------------------+------------------------------+-------------------------+--------------------------+---------------------------+\n",
      "|InvoiceDate|Australia_sum(Quantity)|Australia_sum(UnitPrice)|Australia_sum(CustomerID)|Austria_sum(Quantity)|Austria_sum(UnitPrice)|Austria_sum(CustomerID)|Bahrain_sum(Quantity)|Bahrain_sum(UnitPrice)|Bahrain_sum(CustomerID)|Belgium_sum(Quantity)|Belgium_sum(UnitPrice)|Belgium_sum(CustomerID)|Brazil_sum(Quantity)|Brazil_sum(UnitPrice)|Brazil_sum(CustomerID)|Canada_sum(Quantity)|Canada_sum(UnitPrice)|Canada_sum(CustomerID)|Channel Islands_sum(Quantity)|Channel Islands_sum(UnitPrice)|Channel Islands_sum(CustomerID)|Cyprus_sum(Quantity)|Cyprus_sum(UnitPrice)|Cyprus_sum(CustomerID)|Czech Republic_sum(Quantity)|Czech Republic_sum(UnitPrice)|Czech Republic_sum(CustomerID)|Denmark_sum(Quantity)|Denmark_sum(UnitPrice)|Denmark_sum(CustomerID)|EIRE_sum(Quantity)|EIRE_sum(UnitPrice)|EIRE_sum(CustomerID)|European Community_sum(Quantity)|European Community_sum(UnitPrice)|European Community_sum(CustomerID)|Finland_sum(Quantity)|Finland_sum(UnitPrice)|Finland_sum(CustomerID)|France_sum(Quantity)|France_sum(UnitPrice)|France_sum(CustomerID)|Germany_sum(Quantity)|Germany_sum(UnitPrice)|Germany_sum(CustomerID)|Greece_sum(Quantity)|Greece_sum(UnitPrice)|Greece_sum(CustomerID)|Hong Kong_sum(Quantity)|Hong Kong_sum(UnitPrice)|Hong Kong_sum(CustomerID)|Iceland_sum(Quantity)|Iceland_sum(UnitPrice)|Iceland_sum(CustomerID)|Israel_sum(Quantity)|Israel_sum(UnitPrice)|Israel_sum(CustomerID)|Italy_sum(Quantity)|Italy_sum(UnitPrice)|Italy_sum(CustomerID)|Japan_sum(Quantity)|Japan_sum(UnitPrice)|Japan_sum(CustomerID)|Lebanon_sum(Quantity)|Lebanon_sum(UnitPrice)|Lebanon_sum(CustomerID)|Lithuania_sum(Quantity)|Lithuania_sum(UnitPrice)|Lithuania_sum(CustomerID)|Malta_sum(Quantity)|Malta_sum(UnitPrice)|Malta_sum(CustomerID)|Netherlands_sum(Quantity)|Netherlands_sum(UnitPrice)|Netherlands_sum(CustomerID)|Norway_sum(Quantity)|Norway_sum(UnitPrice)|Norway_sum(CustomerID)|Poland_sum(Quantity)|Poland_sum(UnitPrice)|Poland_sum(CustomerID)|Portugal_sum(Quantity)|Portugal_sum(UnitPrice)|Portugal_sum(CustomerID)|RSA_sum(Quantity)|RSA_sum(UnitPrice)|RSA_sum(CustomerID)|Saudi Arabia_sum(Quantity)|Saudi Arabia_sum(UnitPrice)|Saudi Arabia_sum(CustomerID)|Singapore_sum(Quantity)|Singapore_sum(UnitPrice)|Singapore_sum(CustomerID)|Spain_sum(Quantity)|Spain_sum(UnitPrice)|Spain_sum(CustomerID)|Sweden_sum(Quantity)|Sweden_sum(UnitPrice)|Sweden_sum(CustomerID)|Switzerland_sum(Quantity)|Switzerland_sum(UnitPrice)|Switzerland_sum(CustomerID)|USA_sum(Quantity)|USA_sum(UnitPrice)|USA_sum(CustomerID)|United Arab Emirates_sum(Quantity)|United Arab Emirates_sum(UnitPrice)|United Arab Emirates_sum(CustomerID)|United Kingdom_sum(Quantity)|United Kingdom_sum(UnitPrice)|United Kingdom_sum(CustomerID)|Unspecified_sum(Quantity)|Unspecified_sum(UnitPrice)|Unspecified_sum(CustomerID)|\n",
      "+-----------+-----------------------+------------------------+-------------------------+---------------------+----------------------+-----------------------+---------------------+----------------------+-----------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+--------------------+---------------------+----------------------+-----------------------------+------------------------------+-------------------------------+--------------------+---------------------+----------------------+----------------------------+-----------------------------+------------------------------+---------------------+----------------------+-----------------------+------------------+-------------------+--------------------+--------------------------------+---------------------------------+----------------------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+-------------------+--------------------+---------------------+-------------------+--------------------+---------------------+---------------------+----------------------+-----------------------+-----------------------+------------------------+-------------------------+-------------------+--------------------+---------------------+-------------------------+--------------------------+---------------------------+--------------------+---------------------+----------------------+--------------------+---------------------+----------------------+----------------------+-----------------------+------------------------+-----------------+------------------+-------------------+--------------------------+---------------------------+----------------------------+-----------------------+------------------------+-------------------------+-------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-------------------------+--------------------------+---------------------------+-----------------+------------------+-------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------+-----------------------------+------------------------------+-------------------------+--------------------------+---------------------------+\n",
      "| 2011-10-07|                   null|                    null|                     null|                 null|                  null|                   null|                 null|                  null|                   null|                 null|                  null|                   null|                null|                 null|                  null|                null|                 null|                  null|                         null|                          null|                           null|                 345|   136.51000000000002|                360180|                         325|                        49.38|                        127810|                  637|                 27.69|                  74364|               448| 225.09999999999997|              760461|                            null|                             null|                              null|                 null|                  null|                   null|                 527|    88.43999999999998|                483504|                 2053|    327.79999999999995|                1150698|                null|                 null|                  null|                   null|                    null|                     null|                 null|                  null|                   null|                null|                 null|                  null|               null|                null|                 null|               null|                null|                 null|                 null|                  null|                   null|                   null|                    null|                     null|               null|                null|                 null|                      212|                     25.35|                     140558|                null|                 null|                  null|                null|                 null|                  null|                  null|                   null|                    null|             null|              null|               null|                      null|                       null|                        null|                   null|                    null|                     null|                227|  123.08000000000001|               275880|                null|                 null|                  null|                     null|                      null|                       null|             null|              null|               null|                              null|                               null|                                null|                       25657|           12425.409999999976|                      27566889|                     null|                      null|                       null|\n",
      "| 2011-05-06|                   null|                    null|                     null|                   42|                 58.95|                  74484|                 null|                  null|                   null|                  182|                 63.32|                 260274|                null|                 null|                  null|                null|                 null|                  null|                         null|                          null|                           null|                null|                 null|                  null|                        null|                         null|                          null|                 null|                  null|                   null|              1694|              75.37|              283120|                            null|                             null|                              null|                 null|                  null|                   null|                null|                 null|                  null|                  222|    12.639999999999999|                 126857|                null|                 null|                  null|                   null|                    null|                     null|                 null|                  null|                   null|                null|                 null|                  null|               null|                null|                 null|               null|                null|                 null|                 null|                  null|                   null|                   null|                    null|                     null|               null|                null|                 null|                     null|                      null|                       null|                null|                 null|                  null|                null|                 null|                  null|                  null|                   null|                    null|             null|              null|               null|                      null|                       null|                        null|                   null|                    null|                     null|                 88|               113.1|               187530|                null|                 null|                  null|                     null|                      null|                       null|             null|              null|               null|                              null|                               null|                                null|                       17404|            6952.629999999985|                      18722445|                     null|                      null|                       null|\n",
      "| 2011-01-30|                   null|                    null|                     null|                 null|                  null|                   null|                 null|                  null|                   null|                 null|                  null|                   null|                null|                 null|                  null|                null|                 null|                  null|                         null|                          null|                           null|                null|                 null|                  null|                        null|                         null|                          null|                 null|                  null|                   null|              null|               null|                null|                            null|                             null|                              null|                 null|                  null|                   null|                null|                 null|                  null|                 null|                  null|                   null|                null|                 null|                  null|                   null|                    null|                     null|                 null|                  null|                   null|                null|                 null|                  null|               null|                null|                 null|               null|                null|                 null|                 null|                  null|                   null|                   null|                    null|                     null|               null|                null|                 null|                     null|                      null|                       null|                null|                 null|                  null|                null|                 null|                  null|                  null|                   null|                    null|             null|              null|               null|                      null|                       null|                        null|                   null|                    null|                     null|               null|                null|                 null|                null|                 null|                  null|                     null|                      null|                       null|             null|              null|               null|                              null|                               null|                                null|                        3367|            2321.720000000002|                      11334037|                     null|                      null|                       null|\n",
      "+-----------+-----------------------+------------------------+-------------------------+---------------------+----------------------+-----------------------+---------------------+----------------------+-----------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+--------------------+---------------------+----------------------+-----------------------------+------------------------------+-------------------------------+--------------------+---------------------+----------------------+----------------------------+-----------------------------+------------------------------+---------------------+----------------------+-----------------------+------------------+-------------------+--------------------+--------------------------------+---------------------------------+----------------------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------------+---------------------+----------------------+-----------------------+--------------------+---------------------+----------------------+-------------------+--------------------+---------------------+-------------------+--------------------+---------------------+---------------------+----------------------+-----------------------+-----------------------+------------------------+-------------------------+-------------------+--------------------+---------------------+-------------------------+--------------------------+---------------------------+--------------------+---------------------+----------------------+--------------------+---------------------+----------------------+----------------------+-----------------------+------------------------+-----------------+------------------+-------------------+--------------------------+---------------------------+----------------------------+-----------------------+------------------------+-------------------------+-------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-------------------------+--------------------------+---------------------------+-----------------+------------------+-------------------+----------------------------------+-----------------------------------+------------------------------------+----------------------------+-----------------------------+------------------------------+-------------------------+--------------------------+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.show(3)\n",
    "dfWithDate.printSchema()\n",
    "dfWithDate = dfWithDate.withColumn('Quantity', col('Quantity').cast('long'))\n",
    "dfWithDate = dfWithDate.withColumn('CustomerID', col('CustomerID').cast('long'))\n",
    "dfWithDate.groupBy('InvoiceDate').pivot('Country').sum().printSchema()\n",
    "dfWithDate.groupBy('InvoiceDate').pivot('Country').sum().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
