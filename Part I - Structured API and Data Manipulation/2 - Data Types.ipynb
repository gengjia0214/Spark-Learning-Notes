{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - Data Types\n",
    "Jia Geng | gjia0214@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='directory'></a>\n",
    "\n",
    "## Directory\n",
    "\n",
    "- [Data Source](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/)\n",
    "- [1. Boolean & Filtering](#sec1)\n",
    "- [2.1 Numbers](#sec2-1)\n",
    "- [2.2 More Useful Functions](#sec2-2)\n",
    "- [3. Strings](#sec3)\n",
    "- [4. Date and Timestamp](#sec4)\n",
    "- [5. Nulls](#sec5)\n",
    "- [6. Copmlex Types](#sec6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Locations\n",
    "\n",
    "**Column Methods**\n",
    "- `pyspark.sql.functions`\n",
    "\n",
    "**DataFrame methods**: Since DataFrame is just a Dataset of Row objects. Many useful methods are under the `DataSet` module:\n",
    "- sub-module `DataFrameStatFunctions` for statistical methods\n",
    "- sub-module `DataFrameNaFunctions` for handling null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\r\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~19.10-b09)\r\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "# check java version \n",
    "# use sudo update-alternatives --config java \n",
    "# to switch java version if needed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "data_example_path = '/home/jgeng/Documents/Git/SparkLearning/data/retail.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fda5cb35210>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a spark session locally\n",
    "spark = SparkSession.builder.appName('Spark Example').getOrCreate()\n",
    "\n",
    "# specify the number of worker\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Boolean / Filtering <a id='sec1'></a>\n",
    "\n",
    "Booleans are usually for filtering and sometime we need to create boolean column to filter the data.\n",
    "\n",
    "Some methods for boolean selection/filter from `pyspark.sql.functions`.\n",
    "Below functions return a Column object, which contains the index information for filtering.\n",
    "Some column objects are not boolean, need to convert the column object to boolean. Because `filter` and `where` only accept boolean.\n",
    "- `df.col_name.isin(s or [s1, s2])`: return booleans can be directly used for filtering.\n",
    "- `instr(col_name, s)`: return the position of the first occurance of the substring s in data (0 if not occur), need to convert to boolean for filtering.\n",
    "- if use variable for filtering expression, need to use `&` `|` for `and` `or` operation\n",
    "- **Be careful about null data when creating boolean**. Use `.eqNullSafe()` (more later)\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|    71053|WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536373|    71053|WHITE METAL LANTERN|       6|2010-12-01 09:02:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536375|    71053|WHITE METAL LANTERN|       6|2010-12-01 09:32:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536396|    71053|WHITE METAL LANTERN|       6|2010-12-01 10:51:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536406|    71053|WHITE METAL LANTERN|       8|2010-12-01 11:33:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536544|    71053|WHITE METAL LANTERN|       1|2010-12-01 14:32:00|     8.47|      null|United Kingdom|\n",
      "|   536544|      DOT|     DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|     DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering by value in a specific column\n",
    "df.where(df.StockCode.isin(['DOT', '71053'])).show()\n",
    "df.where(df.StockCode.isin('DOT', '71053'))  # equivelent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+-----------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|    Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+-----------+\n",
      "|   536370|     POST|    POSTAGE|       3|2010-12-01 08:45:00|     18.0|   12583.0|     France|\n",
      "|   536403|     POST|    POSTAGE|       1|2010-12-01 11:27:00|     15.0|   12791.0|Netherlands|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "# filter by whether a substring is in a column feature\n",
    "df.where(instr(df.Description, 'POSTAGE')==8).show(2)  # Postage first appear at pos 8 in DOTCOM POSTAGE\n",
    "df.where(instr(df.Description, 'POSTAGE')==1).show(2)  # find all rows with Description field that start with POSTAGE\n",
    "df.where(instr(df.Description, 'POSTAGE')==0).show(2)  # find all rows with Description fielf that does not contain POSTAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|    71053|WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536373|    71053|WHITE METAL LANTERN|       6|2010-12-01 09:02:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combined filters\n",
    "filter1 = instr(df.Description, 'POSTAGE')==0\n",
    "filter2 = df.StockCode.isin(['DOT', '71053'])\n",
    "df.where(filter1 | filter2).show(2)\n",
    "df.where(filter1 & filter2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+\n",
      "|StockCode|Quantity|UnitPrice|\n",
      "+---------+--------+---------+\n",
      "|      DOT|       1|   569.77|\n",
      "|      DOT|       1|   607.49|\n",
      "+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# we can also use withColumn() to do the filtering and selection together\n",
    "# Adds a boolean column -> filter on the boolean column -> select\n",
    "df.withColumn('isExpensive', col('UnitPrice')>200).where('isExpensive').select('StockCode', 'Quantity', 'UnitPrice').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Numbers <a id='sec2-1'></a>\n",
    "\n",
    "Often time we need to manually transform the columns with some defined functions. We can do this by getting the numeric column (`df.col_name`) then apply regular operations on it then add it to the table (use `select` or `withColumn`).\n",
    "- operations between two numerical column is supported.\n",
    "- `selectExpr(expressions)` or `select(expr())` can also do this\n",
    "- use `round(num, digits)` to round number or ROUND in Expr\n",
    "- `round` will round 2.5 to 3. `bround` will round 2.5 to 2 \n",
    "- `corr` to compute correlation between two columns (with `selectExpr`). \n",
    "    - correlation between two column is a singal value so don't `select` it with other column\n",
    "    - use `df.stat.corr(col1, col2)` to get scalar values\n",
    "    \n",
    "**A very handy method: `df.describe().show()` to show some basic statistic about the data.**\n",
    "If want to get the specific metric, use functions from `pyspark.sql.functions`: `count()`, `mean()`, `stdev_pop()`, `min()`, `max()`\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+----------+--------------+-----------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|CustomerID|       Country|    TrueUnitPrice|\n",
      "+---------+---------+--------------------+--------+-------------------+----------+--------------+-----------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|   17850.0|United Kingdom|            50.41|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|   17850.0|United Kingdom|77.08840000000002|\n",
      "+---------+---------+--------------------+--------+-------------------+----------+--------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modified price\n",
    "correctPrice = pow((df.UnitPrice * 2 + 2), 2)\n",
    "\n",
    "# DO NOT drop before select\n",
    "# we need the column when compute correctPrice\n",
    "df.select('*', correctPrice.alias('TrueUnitPrice')).drop('UnitPrice').show(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|StockCode|        TotalPrice|\n",
      "+---------+------------------+\n",
      "|   85123A|15.299999999999999|\n",
      "|    71053|             20.34|\n",
      "+---------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiply two numerical is supported \n",
    "totalPrice = df.Quantity * df.UnitPrice\n",
    "df.select('StockCode', totalPrice.alias('TotalPrice')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|StockCode|TotalPrice|\n",
      "+---------+----------+\n",
      "|   85123A|      15.3|\n",
      "|    71053|     20.34|\n",
      "+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# above can be executed using the selectExpr\n",
    "df.selectExpr('StockCode', 'ROUND(SQRT(POWER(Quantity * UnitPrice, 2)), 2) as TotalPrice').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|corr(CAST(Quantity AS DOUBLE), UnitPrice)|\n",
      "+-----------------------------------------+\n",
      "|                     -0.04112314436835551|\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.04112314436835551"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between two columns\n",
    "df.selectExpr('corr(Quantity, UnitPrice)').show()\n",
    "\n",
    "# use corr from df.stat\n",
    "df.stat.corr('Quantity', 'UnitPrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|        CustomerID|          Quantity|         UnitPrice|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              1968|              3108|              3108|\n",
      "|   mean|15661.388719512195| 8.627413127413128| 4.151946589446603|\n",
      "| stddev|1854.4496996893627|26.371821677029203|15.638659854603892|\n",
      "|    min|           12431.0|               -24|               0.0|\n",
      "|    max|           18229.0|               600|            607.49|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# super handy method  \n",
    "df.select('CustomerID', 'Quantity', 'UnitPrice').describe().show()  # becarefule about the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|   avg(UnitPrice)|stddev_pop(UnitPrice)|\n",
      "+-----------------+---------------------+\n",
      "|4.151946589446603|   15.636143780280698|\n",
      "+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev_pop, mean\n",
    "\n",
    "df.select(mean('UnitPrice'), stddev_pop('UnitPrice')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Some Other Useful functions <a id='sec2-2'></a>\n",
    "\n",
    "- `df.stat.crosstab(col1, col2)` return a frequency table of paired feature values.\n",
    "- `freqItems()` to find frequent items for columns, possibly with false positives.\n",
    "- `df.stat.approxQuantile(colname, [quantiles], relError)` to get the approximated quantile\n",
    "- `monotonically_increasing_id` from `pyspark.sql.functions` to introduce id into data frame (using `select`)\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----+----+----+----+----+----+\n",
      "|CustomerID_UnitPrice|1.69|1.85|2.55|2.75|3.39|4.25|7.65|\n",
      "+--------------------+----+----+----+----+----+----+----+\n",
      "|             13047.0|   1|   0|   0|   0|   0|   0|   0|\n",
      "|             17850.0|   0|   2|   1|   1|   3|   1|   1|\n",
      "+--------------------+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross tab\n",
    "df.limit(10).stat.crosstab('CustomerID', 'UnitPrice').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|CustomerID_freqItems|\n",
      "+--------------------+\n",
      "|[12662.0, 12868.0...|\n",
      "+--------------------+\n",
      "\n",
      "15\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# freq items\n",
    "df.stat.freqItems(['CustomerID']).show()\n",
    "print(df.where('CustomerID = 12662').count())\n",
    "print(df.where('CustomerID = 12868').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "| id|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|  0|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|  1|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|  2|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|  3|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|  4|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# add id to a dataframe\n",
    "df.select(monotonically_increasing_id().alias('id'), '*').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    avg(Quantity)|\n",
      "+-----------------+\n",
      "|8.627413127413128|\n",
      "+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|min(Quantity)|\n",
      "+-------------+\n",
      "|          -24|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|max(Quantity)|\n",
      "+-------------+\n",
      "|          600|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-24.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 6.0, 10.0, 12.0, 600.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # must be all float!!!\n",
    "relError = 0.01\n",
    "df.select(mean('Quantity')).show()\n",
    "df.selectExpr('min(Quantity)').show()\n",
    "df.selectExpr('max(Quantity)').show()\n",
    "\n",
    "# approximated quantiles\n",
    "\n",
    "df.stat.approxQuantile('Quantity', quantiles, relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Strings <a id='sec3'></a>\n",
    "\n",
    "Case, trimming, padding: under `pyspark.sql.functions`:\n",
    "- `initcap(col(col_name))` to capitalize the initial of each word\n",
    "    - similarly, `lower`, `upper`\n",
    "- `ltrim`, `rtrim`, `trim` to trim spaces\n",
    "- `lpad(col(col_name), n, s)`, to left pad the column with n given string. `rpad` for right pad \n",
    "\n",
    "Oftentime we need to do some fine feature engineering on the string data. `pyspark.sql.functions` have some string manipulation method supports regular exptression (regex). Note: all methods return a column object\n",
    "- replace regex with a given string: `regexp_replace(col(col_name), regex_string, s)` \n",
    "- translate via mapping: `translate(col(col_name), 'ABC', 'abc')` A -> a, B -> b, C -> c. \n",
    "- extract using regex: `regexp_extrac(col(col_name), regex_string, i)`\n",
    "\n",
    "Check whether contains a string\n",
    "- use `instr(col(col_name), substring) >= 1` to create a boolean column \n",
    "    - `instr` return a column of integers for the position indexes of the first matched substring.\n",
    "    - use `>=1` to convert it to boolean column\n",
    "    - use withColumn -> where -> select to do the filtering \n",
    "    \n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example_path)\n",
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|white hanging hea...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap, lower, upper\n",
    "\n",
    "# init capitalize\n",
    "df.select(initcap(col('Description')).alias('Description')).show(1)\n",
    "# lower\n",
    "df.select(lower(col('Description')).alias('Description')).show(1)\n",
    "# upper\n",
    "df.select(upper(col('Description')).alias('Description')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|  Not Trimed|\n",
      "+------------+\n",
      "|     hello  |\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------+\n",
      "|Trimed|\n",
      "+------+\n",
      "| hello|\n",
      "+------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+\n",
      "|    Padded|\n",
      "+----------+\n",
      "|xxxxxhello|\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim, lpad, lit\n",
    "\n",
    "# trim    \n",
    "df.select(lit('     hello  ').alias('Not Trimed')).show(1)\n",
    "df.select(trim(lit('     hello  ')).alias('Trimed')).show(1)\n",
    "\n",
    "# pad\n",
    "df.select(lpad(lit('hello'), 10, \"x\").alias('Padded')).show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         DESCRIPTION|\n",
      "+--------------------+\n",
      "|COLOR HANGING HEA...|\n",
      "| COLOR METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+\n",
      "|         DESCRIPTION|\n",
      "+--------------------+\n",
      "|12ITE 2ANGING 2EA...|\n",
      "| 12ITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+\n",
      "|Extracted|\n",
      "+---------+\n",
      "|    WHITE|\n",
      "|    WHITE|\n",
      "+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace, translate\n",
    "\n",
    "regex_string = 'BLACK|WHITE|GRAY|RED|GREEN|BLUE'  # any regex style string, here | is regex or\n",
    "\n",
    "# replace\n",
    "df.select(regexp_replace(col('DESCRIPTION'), regex_string, 'COLOR').alias('DESCRIPTION')).show(2)  # replace all colors by COLOR\n",
    "\n",
    "\n",
    "# translate\n",
    "df.select(translate(col('DESCRIPTION'), 'WH', '12').alias('DESCRIPTION')).show(2)\n",
    "\n",
    "\n",
    "# extract \n",
    "regex_string = '(BLACK|WHITE|GRAY|RED|GREEN|BLUE)'\n",
    "df.select(regexp_extract(col('DESCRIPTION'), regex_string, 1).alias('Extracted')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "# boolean column of whether contain WHITE\n",
    "containWHITE = instr(df.Description, 'WHITE') >= 1\n",
    "\n",
    "# use the withColumn, where, select trick to filter \n",
    "df.withColumn('containWhite', containWHITE).where('containWhite').select('Description').show(3, False)  # False to print all info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+--------+-------+\n",
      "|is_white|is_black|is_red|is_green|is_blue|\n",
      "+--------+--------+------+--------+-------+\n",
      "|    true|   false| false|   false|  false|\n",
      "|    true|   false| false|   false|  false|\n",
      "|   false|   false| false|   false|  false|\n",
      "+--------+--------+------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|JUMBO  BAG BAROQU...|\n",
      "|WOOD BLACK BOARD ...|\n",
      "|JUMBO  BAG BAROQU...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "\n",
    "# A good compact color keyword filtering\n",
    "colors = ['white', 'black', 'red', 'green', 'blue']\n",
    "\n",
    "def color_locator(column, color_string):\n",
    "    \"\"\"\n",
    "    method to return a boolean column of whether contain the color\n",
    "    \"\"\"\n",
    "    # name the column is is_{} so that we can use expression to filter designated color\n",
    "    # cast the location (1-based) to boolean: 0 -> False >=1 -> True\n",
    "    return locate(color_string.upper(), column).cast('boolean').alias('is_{}'.format(color_string))\n",
    "\n",
    "# get all color boolean columns!\n",
    "all_color_location = [color_locator(df.Description, color) for color in colors]\n",
    "\n",
    "# what it looks like\n",
    "df.select(*all_color_location).show(3)\n",
    "\n",
    "# select all descriptions that contains color white and black\n",
    "# use where('col_name1 and col_name2') to filter color since these are boolean column\n",
    "# first select need to include the description column! \n",
    "df.select(*all_color_location, 'Description').where('is_white and is_black').select('Description').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dates and Timestamps <a id='sec4'></a>\n",
    "\n",
    "Most of the time, we do:\n",
    "- convert strings to date/timestamps after loading the string data\n",
    "- extract additional features from date/timestamps - holidays, i-th day of week/month, is weekend, etc\n",
    "- encode date/timestamps for the machine learning models (not discussed here but later)\n",
    "\n",
    "Spark use `TimestampType` class\n",
    "\n",
    "Some potential GOTCHAs:\n",
    "- spark might not be able to recognize the date/timestamp from the data, when strangely formatted\n",
    "- spark `TimestampType` only supports second-level precision. If we need to work with ms or us, need to operating them as `longs`!\n",
    "\n",
    "**Some Essential Functions for timestamp (from `pyspark.sql.functions`)**\n",
    "- create date/time: `current_date()`, `current_timestamp()`, `to_date(lit('MM-dd-yyyy'))`, `to_timestampe(lit(...))`\n",
    "    - `to_date()`, `to_timestamp()`converts a lit(string) to date. **if the input can not be converted, it will create a null value!**\n",
    "    - this can be fixed by specifying the date format, e.g. `to_date(lit(...), 'yyyy-MM-dd')`\n",
    "    - **MM for month, mm for minutes!!!!**\n",
    "- operations: `date_sub(col, n)`, `date_add(col, n)`\n",
    "- compute gap: `datediff(col1, col2)`, `months_between(col1, col2)`\n",
    "\n",
    "\n",
    "**One big GOTCHA**\n",
    "- the input data might not always follow the correct format. it is possible some are, some aren't.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example_path)\n",
    "df.show(3)\n",
    "df.printSchema()  # when the data is in standard format of data time, it will be able to infer it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |date      |time                   |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2020-04-24|2020-04-24 14:37:40.607|\n",
      "|1  |2020-04-24|2020-04-24 14:37:40.607|\n",
      "|2  |2020-04-24|2020-04-24 14:37:40.607|\n",
      "+---+----------+-----------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- date: date (nullable = false)\n",
      " |-- time: timestamp (nullable = false)\n",
      "\n",
      "+----------+\n",
      "|      bday|\n",
      "+----------+\n",
      "|1992-02-14|\n",
      "|1992-02-14|\n",
      "|1992-02-14|\n",
      "+----------+\n",
      "\n",
      "+-------------------+\n",
      "|               bday|\n",
      "+-------------------+\n",
      "|1992-02-14 00:00:00|\n",
      "|1992-02-14 00:00:00|\n",
      "|1992-02-14 00:00:00|\n",
      "+-------------------+\n",
      "\n",
      "+---------+\n",
      "|incorrect|\n",
      "+---------+\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, to_date, lit, to_timestamp\n",
    "\n",
    "# create a datetime dataframe\n",
    "# spark.range will have a automatic column name: id!!\n",
    "\n",
    "datetimeDF = spark.range(3).withColumn('date', current_date()).withColumn('time', current_timestamp())\n",
    "datetimeDF.show(4, False)\n",
    "datetimeDF.printSchema()\n",
    "\n",
    "# to date converts a string to date (need to use lit())\n",
    "datetimeDF.select(to_date(lit('1992-02-14'), 'yyyy-MM-dd').alias('bday')).show()  # MM for month, mm for minutes!\n",
    "datetimeDF.select(to_timestamp(lit('1992-02-14'), 'yyyy-MM-dd').alias('bday')).show()  # MM for month, mm for minutes!\n",
    "datetimeDF.select(to_date(lit('1992-20-02')).alias('incorrect')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+-----------+------------+\n",
      "|id |date      |time                   |week before|month before|\n",
      "+---+----------+-----------------------+-----------+------------+\n",
      "|0  |2020-04-24|2020-04-24 14:37:40.867|2020-04-17 |2020-04-09  |\n",
      "|1  |2020-04-24|2020-04-24 14:37:40.867|2020-04-17 |2020-04-09  |\n",
      "|2  |2020-04-24|2020-04-24 14:37:40.867|2020-04-17 |2020-04-09  |\n",
      "+---+----------+-----------------------+-----------+------------+\n",
      "\n",
      "+--------+\n",
      "|day diff|\n",
      "+--------+\n",
      "|       7|\n",
      "|       7|\n",
      "|       7|\n",
      "+--------+\n",
      "\n",
      "+----------+\n",
      "|month diff|\n",
      "+----------+\n",
      "|0.48387097|\n",
      "|0.48387097|\n",
      "|0.48387097|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, datediff, months_between, col\n",
    "\n",
    "# add two columns\n",
    "datetimeDF = datetimeDF.withColumn('week before', date_sub(col('date'), 7))\n",
    "datetimeDF = datetimeDF.withColumn('month before', date_sub(col('date'), 15))\n",
    "datetimeDF.show(20, False)\n",
    "\n",
    "# compute gap\n",
    "datetimeDF.select(datediff(col('date'), col('week before')).alias('day diff')).show()\n",
    "datetimeDF.select(months_between(col('date'), col('month before')).alias('month diff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use date for filtering!\n",
    "df.where(col('invoiceDate') < lit('2010-12-01 08:35:00')).where(col('invoiceDate') > lit('2010-12-01 08:33:00')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Nulls <a id='sec5'></a>\n",
    "\n",
    "In spark, it is better to use null to represent the missing data instead of empty string.\n",
    "\n",
    "Two basic operation on null values:\n",
    "- Drop nulls\n",
    "- Fill nulls (on global or per column basis)\n",
    "\n",
    "Use `coalesce()` to fill the null:\n",
    "- `df.select(coalesce(col(col_name), lit(0))`\n",
    "    - introduce a dummy column of 0s\n",
    "    - coalesce a column with the dummy column\n",
    "    - so that the nulls will be filled by 0s\n",
    "    \n",
    "More common way to deal with null is through the `df.na` field\n",
    "- `df.na.drop(mode[, subset=])`\n",
    "    - `.drop('any')` to drop any row that contains null\n",
    "    - `.drop('all')` to drop any row that is all null and any col that is all null\n",
    "    - `.drop(mode, subset=[feature1, ...])` work on a subset of features\n",
    "- `df.na.fill(fill_val[, subet=])`\n",
    "    - if `fill_val` is does not match, null stays\n",
    "    - `fill_val` can be a dictionary: `{col: val}`\n",
    "- `df.na.replace(to_replace=[...], values=[...], subset=[cols])` is alias of `df.replace()` \n",
    "\n",
    "\n",
    "Ordering with null value `df.orderBy()`\n",
    "- by default null at start\n",
    "- use `asc_nulls_first(col_name)`, `asc_null_last(col_name)`, etc to determine the position of the null values (under `pyspark.sql.functions`)\n",
    "- e.g. `df.orderBy(dsc_null_last(col_name))`\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|null|null|\n",
      "|   1|null|\n",
      "|null|   2|\n",
      "+----+----+\n",
      "\n",
      "+--------------+\n",
      "|coalesce(a, b)|\n",
      "+--------------+\n",
      "|          null|\n",
      "|             1|\n",
      "|             2|\n",
      "+--------------+\n",
      "\n",
      "+----+----+----------------+\n",
      "|   a|   b|coalesce(a, 0.0)|\n",
      "+----+----+----------------+\n",
      "|null|null|             0.0|\n",
      "|   1|null|             1.0|\n",
      "|null|   2|             0.0|\n",
      "+----+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## from pyspark.sql.functions import coalesce\n",
    "\n",
    "# use coalesce fill null (might not be the best way to fill null)\n",
    "\n",
    "# coalesce - for each row, return the first column that is not null\n",
    "cDF = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    "cDF.show()\n",
    "\n",
    "# add a dummy column 0 then coalesce ==> fill null with 0!\n",
    "cDF.select(coalesce(cDF[\"a\"], cDF[\"b\"])).show()\n",
    "cDF.select('*', coalesce(cDF[\"a\"], lit(0.0))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n",
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|   1|null|\n",
      "|null|   2|\n",
      "+----+----+\n",
      "\n",
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "|null|  2|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# managing nulls via df.na attribute functions\n",
    "\n",
    "# drop any - default is drop any row that any value is null\n",
    "cDF.na.drop().show(3)\n",
    "cDF.na.drop('any').show(3)  # equivalent\n",
    "\n",
    "# drop all - drop the row if all values are null, the column if all values are null\n",
    "cDF.na.drop('all').show(3)\n",
    "\n",
    "# drop specific columns, subset=[cols]\n",
    "cDF.na.drop('any', subset=['b']).show(3)  # work on the subset columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|null|null|\n",
      "|   1|null|\n",
      "|null|   2|\n",
      "+----+----+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "| 22| 22|\n",
      "|  1| 22|\n",
      "| 22|  2|\n",
      "+---+---+\n",
      "\n",
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "|null|  2|\n",
      "|   1|  2|\n",
      "|null|  2|\n",
      "+----+---+\n",
      "\n",
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "|null|  2|\n",
      "|   1|  2|\n",
      "|null|  2|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# fill - by default fill all nulls with the given val\n",
    "# if type does not match, stay null\n",
    "cDF.na.fill('22').show(3)  \n",
    "cDF.na.fill(22).show(3)  \n",
    "\n",
    "# fill - subset\n",
    "cDF.na.fill(2, subset=['b']).show()\n",
    "\n",
    "# fill - using dict\n",
    "fill_dict = {'b': 2}\n",
    "cDF.na.fill(fill_dict).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|null|null|\n",
      "| 100|null|\n",
      "|null| 200|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repalce\n",
    "\n",
    "# cDF.na.replace and cDF.replace are alias of each other!\n",
    "cDF.replace([1, 2], [100, 200], subset=['a', 'b']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|null|null|\n",
      "|null|   2|\n",
      "|   1|null|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "|   1|null|\n",
      "|null|   2|\n",
      "|null|null|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc_nulls_last, asc, col\n",
    "\n",
    "# ordering - by default is null first\n",
    "# use asc_nulls_last to specify the position of null\n",
    "# asc functions recieve column name rather than the column object!\n",
    "\n",
    "cDF.orderBy(asc('a')).show()\n",
    "cDF.orderBy(asc_nulls_last('a')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complex Types <a id='sec6'></a>\n",
    " \n",
    "Spark has three kinds of complex types:\n",
    "- `Struct`: DataFrames within DataFrames\n",
    "    - `struct()` under `pyspark.sql.functions`\n",
    "    - use `df.select(struct(cols))` to create struct column\n",
    "    - **struct column support `getField()` method to get the sub-column!**\n",
    "    - use `df.select(col(complex_col_name.*))` to get the sub DataFrame\n",
    "    \n",
    "- `Array`: Features that need to be represented by a list of stuff, e.g. list of words\n",
    "    - use `split(col(col_name), delimiter)` to create an array column\n",
    "    - array column supports index selection! `df.select(col(arr_col_name)[i])`\n",
    "    - array supports `size()` function: `df.select(size(col(arr_col_name)))`\n",
    "    - array supports `array_contains()` fubction\n",
    "    - some more array functions udner `pyspark.sql.functions`\n",
    "    - **Often time, we want to convert the array of items into rows such that each row contains one item**\n",
    "        - this can be achieved by `explode(arr_col)` function, which takes the array col\n",
    "    - from `array<string>` back to string: `concat_ws(sep, col)`\n",
    "- `Map`: key: val pairs of columns\n",
    "    - `create_map(col1, col2)` from `pyspark.sql.functions`\n",
    "    - the Map column supports key-val queries\n",
    "   \n",
    "- `explode(col)` method can:\n",
    "    - break the array column down, each item will become a row\n",
    "    - convert the key-val map column into two seperate columns\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(data_example_path)\n",
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------------------------+\n",
      "|InvoiceNo|Complex_Struct                                    |\n",
      "+---------+--------------------------------------------------+\n",
      "|536365   |[85123A, WHITE HANGING HEART T-LIGHT HOLDER, 2.55]|\n",
      "|536365   |[71053, WHITE METAL LANTERN, 3.39]                |\n",
      "|536365   |[84406B, CREAM CUPID HEARTS COAT HANGER, 2.75]    |\n",
      "+---------+--------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "# creat table with struct column\n",
    "complexDF = df.select('InvoiceNo', struct(col('StockCode'), col('Description'), col('UnitPrice')).alias('Complex_Struct'))\n",
    "complexDF.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Complex_Struct.Description        |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|Complex_Struct.Description        |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------------------+---------+\n",
      "|StockCode|         Description|UnitPrice|\n",
      "+---------+--------------------+---------+\n",
      "|   85123A|WHITE HANGING HEA...|     2.55|\n",
      "|    71053| WHITE METAL LANTERN|     3.39|\n",
      "|   84406B|CREAM CUPID HEART...|     2.75|\n",
      "+---------+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Struct Column is a sub DataFrame\n",
    "\n",
    "# It have sub-columns under the hood, which can be access by getField() method\n",
    "complexDF.select(col('Complex_Struct').getField('Description')).show(3, False)\n",
    "\n",
    "# Use dot also work!\n",
    "complexDF.select(col('Complex_Struct').Description).show(3, False)\n",
    "\n",
    "# use * to get the sub DataFrame from the complex struct!\n",
    "complexDF.select('Complex_Struct.*').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                 arr|\n",
      "+---+--------------------+\n",
      "|  0|[WHITE, HANGING, ...|\n",
      "|  1|[WHITE, METAL, LA...|\n",
      "|  2|[CREAM, CUPID, HE...|\n",
      "+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+\n",
      "|arr[4]|\n",
      "+------+\n",
      "|HOLDER|\n",
      "|  null|\n",
      "|HANGER|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, size, array_contains, monotonically_increasing_id\n",
    "\n",
    "# create an array column by splitting the description column with delimiter ' '\n",
    "arrDF = df.select(monotonically_increasing_id().alias('id'), split(col('Description'), ' ').alias('arr'))\n",
    "arrDF.show(3)\n",
    "\n",
    "# array column supports index selection!\n",
    "arrDF.select(col('arr')[4]).show(3)  # if nothing on the index, it will be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+\n",
      "| id|                 arr|size(arr)|\n",
      "+---+--------------------+---------+\n",
      "|  0|[WHITE, HANGING, ...|        5|\n",
      "|  1|[WHITE, METAL, LA...|        3|\n",
      "|  2|[CREAM, CUPID, HE...|        5|\n",
      "+---+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can get the length of the array using the size function\n",
    "arrDF.select('*', size(col('arr'))).show(3)  # size does not work on regular column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|contain_WHITE|\n",
      "+-------------+\n",
      "|         true|\n",
      "|         true|\n",
      "|        false|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array contains function\n",
    "splitted_col = split(col('Description'), ' ')\n",
    "df.select(array_contains(splitted_col, 'WHITE').alias('contain_WHITE')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| id|                 arr|    col|\n",
      "+---+--------------------+-------+\n",
      "|  0|[WHITE, HANGING, ...|  WHITE|\n",
      "|  0|[WHITE, HANGING, ...|HANGING|\n",
      "|  0|[WHITE, HANGING, ...|  HEART|\n",
      "|  0|[WHITE, HANGING, ...|T-LIGHT|\n",
      "|  0|[WHITE, HANGING, ...| HOLDER|\n",
      "|  1|[WHITE, METAL, LA...|  WHITE|\n",
      "|  1|[WHITE, METAL, LA...|  METAL|\n",
      "|  1|[WHITE, METAL, LA...|LANTERN|\n",
      "|  2|[CREAM, CUPID, HE...|  CREAM|\n",
      "+---+--------------------+-------+\n",
      "only showing top 9 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# explode the array column \n",
    "arrDF.select('*', explode(col('arr'))).show(9)  # explode break down the array items!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 map|\n",
      "+--------------------+\n",
      "|[0 -> [WHITE, HAN...|\n",
      "|[1 -> [WHITE, MET...|\n",
      "|[2 -> [CREAM, CUP...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+----------------------+\n",
      "|                 map|map[CAST(0 AS BIGINT)]|\n",
      "+--------------------+----------------------+\n",
      "|[0 -> [WHITE, HAN...|  [WHITE, HANGING, ...|\n",
      "|[1 -> [WHITE, MET...|                  null|\n",
      "|[2 -> [CREAM, CUP...|                  null|\n",
      "|[3 -> [KNITTED, U...|                  null|\n",
      "|[4 -> [RED, WOOLL...|                  null|\n",
      "+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+----------------------+\n",
      "|                 map|map[CAST(0 AS BIGINT)]|\n",
      "+--------------------+----------------------+\n",
      "|[0 -> [WHITE, HAN...|  [WHITE, HANGING, ...|\n",
      "+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# map so that can access by keys\n",
    "mapDF = arrDF.select(create_map(col('id'), col('arr')).alias('map'))\n",
    "mapDF.show(3)\n",
    "\n",
    "# access by key - this will give you a column\n",
    "# all rows that can not be mapped with the key will be null\n",
    "mapDF.select('*', col('map')[0]).show(5)\n",
    "mapDF.select('*', col('map')[0]).na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------+-----------------------------------------+\n",
      "|Description                       |Quantity|map                                      |\n",
      "+----------------------------------+--------+-----------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|6       |[WHITE HANGING HEART T-LIGHT HOLDER -> 6]|\n",
      "|WHITE METAL LANTERN               |6       |[WHITE METAL LANTERN -> 6]               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |8       |[CREAM CUPID HEARTS COAT HANGER -> 8]    |\n",
      "+----------------------------------+--------+-----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another create map example on non complex column\n",
    "mapDF = df.select('Description', 'Quantity', create_map(col('Description'), col('Quantity')).alias('map'))\n",
    "mapDF.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+------------------------+\n",
      "|         Description|Quantity|                 map|map[WHITE METAL LANTERN]|\n",
      "+--------------------+--------+--------------------+------------------------+\n",
      "|WHITE HANGING HEA...|       6|[WHITE HANGING HE...|                    null|\n",
      "| WHITE METAL LANTERN|       6|[WHITE METAL LANT...|                       6|\n",
      "|CREAM CUPID HEART...|       8|[CREAM CUPID HEAR...|                    null|\n",
      "+--------------------+--------+--------------------+------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapDF.select('*', col('map')['WHITE METAL LANTERN']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|WHITE HANGING HEA...|    6|\n",
      "| WHITE METAL LANTERN|    6|\n",
      "|CREAM CUPID HEART...|    8|\n",
      "|KNITTED UNION FLA...|    6|\n",
      "|RED WOOLLY HOTTIE...|    6|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use explode on map column\n",
    "# explode converts the key-val column into two columns\n",
    "mapDF.select(explode(col('map'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all schemas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
