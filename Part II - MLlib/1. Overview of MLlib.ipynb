{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Learning Note - MLlib\n",
    "Jia Geng | gjia0214@gmail.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='directory'></a>\n",
    "\n",
    "## Directory\n",
    "\n",
    "- [Data Source](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/)\n",
    "- [1. Some Machine Learning Examples](#sec1)\n",
    "- [2. Classic ML Developmental Stages](#sec2-1)\n",
    "- [3.  Spark MLlib Overview](#sec3)\n",
    "- [4. Simple Example Walk Through](#sec4)\n",
    "    - [4.1 Load the data](#sec4-1)\n",
    "    - [4.2 Transformer - RFomula](#sec4-2)\n",
    "    - [4.3 Estimator](#sec4-3)\n",
    "    - [4.4 Pipeline and GridSearch](#sec4-4)\n",
    "    - [4.5 Tuning (Evaluator and GridSearch)](#sec4-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Some Machine Learning Examples <a id='sec1'></a>\n",
    "\n",
    "Supervised Learning\n",
    "- classification\n",
    "    - predicting disease\n",
    "    - clasifying image\n",
    "- regression\n",
    "    - predicting sales\n",
    "    - predicting number of viewer of a show\n",
    "    \n",
    "Recommendation\n",
    "- movie recommendation\n",
    "- product recommendation\n",
    "\n",
    "Unsupervised Learning\n",
    "- anormaly detection\n",
    "- user segmentation \n",
    "- topic modeling\n",
    "\n",
    "Graph Analysis\n",
    "- fraud prediction\n",
    "    - interesting - account within two hops of fraudulent number might be considered as suspicious\n",
    "- anormaly detection\n",
    "    - e.g. if typically in the data each vertex has ten edges associated with it. given a vertex only has one edge -> possible anormaly\n",
    "- classification\n",
    "    - influencer's network has similar structure\n",
    "- recommendation\n",
    "    - PageRank is a graph algorithm!\n",
    "   \n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classic ML Developmental Stages <a id='sec2'></a>\n",
    "\n",
    "- collect data\n",
    "- clean data\n",
    "- feature engineering\n",
    "- modeling\n",
    "- evaluating and tuning\n",
    "- leveraging model/insights\n",
    "\n",
    "[back to top](#directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark MLlib Overview <a id='sec3'></a>\n",
    "\n",
    "Spark MLlib provide two core packages for machine learning;\n",
    "- `pyspark.ml`: provide high level DataFrames APIs for building machine learning piplines\n",
    "- `pyspark.mllib`: provide low level RDD APIs\n",
    "\n",
    "\n",
    "**Spark MLlib vs Other ML packages**\n",
    "- most of other ml packages are **single machine tools**\n",
    "- when to use MLlib?\n",
    "    - when data is large, use MLlib for feature engineering then use single machine tool for modeling\n",
    "    - when data and model are both large and can not fit on one machine, MLlib makes distributed machine learning very simple\n",
    "- potential disadvantage of MLlib\n",
    "    - When deploying the model, MLlib does not have buildin to serve low-latency predictions from a model\n",
    "    - Might want to export the model to another serving system or custom application to do it\n",
    "    \n",
    "**Spark Structual Types**\n",
    "- Transformers: functions convert raw data in some way\n",
    "- Estimators\n",
    "    - can a a kind of transformer than is initialized data, e.g. normalize data need to get the mean and std from data\n",
    "    - algorithms that allow users to train a model from data\n",
    "- Evaluator: provide insight about how a model performs according to some criteria we specified such as AUC.\n",
    "- Pipeline: a container hat pipelining the process, like the scikit-learn pipeline\n",
    "- **The transformer, estimator and evaluater object classes usually can be initiated as a 'blank' object. Then set up the attribute and configuration later. This makes these classes  support the Pipeline construction and grid search.**\n",
    "\n",
    "**Spark Low Level Data Types**\n",
    "- `from pyspark.ml.linalg import Vectors`\n",
    "- Dense Vector: `Vector.dense(1.0, 2.0, 3.0)`\n",
    "- Spark Vector: `Vector.sparse(size, idx, values)` idx for positions that is not zero\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Example Walk Through <a id='sec4'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load the data <a id='sec4-1'></a>\n",
    "\n",
    "Initialize the spark session, load the data, set up partitions, cahce if needed, and do some exploration such as count, check nulls, summary, etc.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://unknown40A5EF2BBD8A:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MLexample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f444d8dfe90>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "data_example_path = '/home/jgeng/Documents/Git/SparkLearning/data/simple-ml' \n",
    "spark = SparkSession.builder.appName('MLexample').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = spark.read.json(data_example_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- lab: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max, min, avg, stddev_samp\n",
    "\n",
    "# check on schema\n",
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check nulls\n",
    "for col_name in df.columns:\n",
    "    print(df.where('{} is null'.format(col_name)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|color|\n",
      "+-----+\n",
      "|green|\n",
      "|  red|\n",
      "| blue|\n",
      "+-----+\n",
      "\n",
      "+----+\n",
      "| lab|\n",
      "+----+\n",
      "| bad|\n",
      "|good|\n",
      "+----+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|            value1|            value2|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               110|               110|\n",
      "|   mean|14.818181818181818|  21.0914521792258|\n",
      "| stddev|13.305294399193416|10.999588110596887|\n",
      "|    min|                 1|14.386294994851129|\n",
      "|    25%|                 2|14.386294994851129|\n",
      "|    50%|                12|14.386294994851129|\n",
      "|    75%|                16| 38.97187133755819|\n",
      "|    max|                45| 38.97187133755819|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('color')).distinct().show(3)\n",
    "df.select(col('lab')).distinct().show(3)\n",
    "df.select('value1', 'value2').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transformer - RFomula <a id='sec4-2'></a>\n",
    "\n",
    "Most of the machine learning algorithms in MLlib needs the input to be transformed into:\n",
    "- Double for labels\n",
    "- Vector[Double] for features\n",
    "\n",
    "**Use R-liked operator to build a `RFomula` as transformer**\n",
    "- under `pyspark.ml.feature`\n",
    "- `~` sperate the target and terms\n",
    "- `+` to concat/include a feature. \n",
    "    - `+0` to remove the intercept\n",
    "- `-` to remove a term\n",
    "    - `-0` to remove the intercept (same as `+0`)\n",
    "- `:` as the interaction between two feature, i.e. multiplication for numeric values or binarized categorical values\n",
    "- `.` all columns except for the target \n",
    "\n",
    "E.g.\n",
    "`lab~.+color:value1+colr:value2` means\n",
    "- label is the target \n",
    "- model takes all columns except lab column as input\n",
    "- model also takes interaction terms between color:value1, color:value2 as input\n",
    "\n",
    "To transform data into usable features:\n",
    "- build a `RFormula object`\n",
    "- use `RFormula.fit(data_df)` to set up the transform configuration. `fit` return a `RFormulaModel` object\n",
    "- Transform the data via `RFormulaModel` by calling `.transform(data)` \n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFormula_d4badd3d691f"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "# specify the transformer using RFormula\n",
    "rfm = RFormula()\n",
    "rfm.setFormula('lab~.+color:value1+color:value2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.RFormulaModel'>\n",
      "featuresCol: features column name (default: features)\n",
      "forceIndexLabel: Force to index label whether it is numeric or string (default: False)\n",
      "formula: R model formula (current: lab~.+color:value1+color:value2)\n",
      "handleInvalid: How to handle invalid data (unseen or NULL values) in features and label column of string type. Options are 'skip' (filter out rows with invalid data), error (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error)\n",
      "labelCol: label column name (default: label)\n",
      "stringIndexerOrderType: How to order categories of a string FEATURE column used by StringIndexer. The last category after ordering is dropped when encoding strings. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. The default value is 'frequencyDesc'. When the ordering is set to 'alphabetDesc', RFormula drops the same category as R when encoding strings. (default: frequencyDesc)\n"
     ]
    }
   ],
   "source": [
    "# fit the rformula object with data to create the transformer\n",
    "transformer = rfm.fit(df)\n",
    "print(type(transformer))\n",
    "print(transformer.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- lab: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n",
      "+--------------------------------------------------------------------+\n",
      "|features                                                            |\n",
      "+--------------------------------------------------------------------+\n",
      "|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])|\n",
      "|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])      |\n",
      "|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])    |\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform - it will concat a feature column to the original df\n",
    "preparedDF = transformer.transform(df)\n",
    "preparedDF.show(3)\n",
    "preparedDF.printSchema()\n",
    "preparedDF.select('features').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# split the data into train an test\n",
    "train, test = preparedDF.randomSplit([0.7, 0.3])\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Estimator <a id='sec4-3'></a>\n",
    "\n",
    "Most of the algorithms are under `pyspark.ml`. E.g logistic regression under `pyspark.ml.classification.LogisticRegression`\n",
    "\n",
    "The classifier constructor usually takes in parameters that specify the feature column and label column along with some hyperparameters. **MOst classifier object have a funtion `explainParam()` that can provide info regarding the hyperparameters**\n",
    "\n",
    "Estimator\n",
    "- the class object only contains the params configuration for the model, e.g. `LogisticRegreesion`\n",
    "- use `.fit()` to fit the training data\n",
    "    - `fit` returns a trained classifier, e.g. `LogisticRegressionModel`. this is the classifier object that contains weights etc. for making predictions!\n",
    "- use `.transform()` to make predictions since logically, prediction is just transform the input to labels!\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.classification.LogisticRegression'>\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(labelCol='label', featuresCol='features')\n",
    "print(type(logit))\n",
    "print(logit.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.classification.LogisticRegressionModel'>\n"
     ]
    }
   ],
   "source": [
    "# fit the model with training data\n",
    "clf = logit.fit(train)\n",
    "print(type(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[-97.35859712,  80.05075389,   1.17557914,   0.93307193,\n",
      "                3.48110436, -10.31645717,  -9.71304054,  -1.24157784,\n",
      "                3.66830126,  -6.93705856]])\n"
     ]
    }
   ],
   "source": [
    "print(clf.coefficientMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|color|lab|value1|            value2|            features|label|       rawPrediction|         probability|prediction|\n",
      "+-----+---+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[173.722804601959...|[1.0,3.5739179735...|       0.0|\n",
      "| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[173.722804601959...|[1.0,3.5739179735...|       0.0|\n",
      "| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[173.722804601959...|[1.0,3.5739179735...|       0.0|\n",
      "+-----+---+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# making predictions\n",
    "clf.transform(test).show(3) # probably want to select the probability and prediction column only "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Pipeline and GridSearch <a id='sec4-4'></a>\n",
    "\n",
    "Spark also have a pipeline class: `pyspark.ml.Pipeline`. `Pipeline` is essentially a compact estimator that can do feature transformation, model fitting and prediction. `Pipeline` have a `.stages` attribute that keeps the configurations of the transformer and estimator.\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Tuning (Evaluator and GridSearch)  <a id='sec4-5'></a>\n",
    "\n",
    "Spark provide a very compact way to do model selection. \n",
    "\n",
    "Steps:\n",
    "- initialize the transformer, estimator and pipeline \n",
    "- set up the `ParamGridBuilder` under `pyspark.ml.tuning` for the grid search\n",
    "    - `ParamGridBuilder` can be used to configure the searching space for transformer (feature subsets) and the estimator (model hyperparam)\n",
    "    - use `ParamGridBuilder.addGrid(attr, candidates)` to configure the grid search\n",
    "- create a evaluator. `pyspark.ml.evaluation` host different types of evaluators for different task, that can be used for evaluating the model performance. When constructing the evaluator, you usually need to:\n",
    "    - `setMetricName()`\n",
    "    - `setRawPredictionCol()`\n",
    "    - `setLabelCol()`\n",
    "- create a verifier, e.g. `TrainValidationSplit`. this is a compact class that takes in the pipeline and evaluator and do tuning,\n",
    "\n",
    "After training.\n",
    "- use `evaluator.evaluate(tvsFitted.bestModel.transform(test))` for the performance\n",
    "- to check the training record on the best model `summary= tvsFitted.bestModel.stages[1].summary`\n",
    "    - use stages to get the classifer if pipeline estimator was used\n",
    "    - `summary.objectiveHistory` is the loss history during training\n",
    "    - `summary.roc.show()` gives the roc curve data\n",
    "\n",
    "To load/write model, just use `load` `write` mothod\n",
    "\n",
    "[back to top](#directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.pipeline.Pipeline'> <class 'pyspark.ml.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# prepare the transformer and the estimator\n",
    "# do not specify any hyperparameters here\n",
    "rfm = RFormula()\n",
    "logit = LogisticRegression().setLabelCol('label').setFeaturesCol('features')\n",
    "\n",
    "# construct the pipeline\n",
    "ppBuilder = Pipeline()\n",
    "\n",
    "# set up the stage\n",
    "stages = [rfm, logit]\n",
    "pp = ppBuilder.setStages(stages)  # does not configure inplace!\n",
    "print(type(ppBuilder), type(pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# build the evaluator\n",
    "evaluator = BinaryClassificationEvaluator().setMetricName('areaUnderROC')\\\n",
    "                                            .setRawPredictionCol('prediction')\\\n",
    "                                            .setLabelCol('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# building the grid search space\n",
    "rfm_can = ['lab~.', 'lab~.+color:value1+color:value2']  # feature space\n",
    "enet_can = [0, 0.5, 1] # 0 for l1 0.5 for l1 l2, 1 for l2\n",
    "reg_can = [0, 1e-3, 1e-2, 1e-1, 1, 10] # 0 for no regularization\n",
    "params = ParamGridBuilder().addGrid(rfm.formula, rfm_can)\\\n",
    "                            .addGrid(logit.elasticNetParam, enet_can)\\\n",
    "                            .addGrid(logit.regParam, reg_can)\\\n",
    "                            .build()  # dont forget to call build!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "# build the train validation machine\n",
    "# tvs will takes 0.25 of the training data as the holdout set for validation\n",
    "tvs = TrainValidationSplit().setTrainRatio(0.75)\\\n",
    "                            .setEstimator(pp)\\\n",
    "                            .setEvaluator(evaluator)\\\n",
    "                            .setEstimatorParamMaps(params)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "tvsFitted = tvs.fit(train)  # train on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.bestModel.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "36\n",
      "[0.8444444444444443, 0.8444444444444443, 0.8444444444444443, 0.9, 0.6, 0.5, 0.8444444444444443, 0.8444444444444443, 0.8444444444444443, 0.9, 0.5, 0.5, 0.8444444444444443, 0.8444444444444443, 0.8444444444444443, 0.9, 0.5, 0.5, 1.0, 1.0, 0.9, 0.9, 0.8, 0.5, 1.0, 1.0, 0.9, 0.9, 0.5, 0.5, 1.0, 1.0, 0.9, 0.9, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# get the valdiation results via the validation metrics\n",
    "# get associated model params via getEstimatorParamMaps\n",
    "print(len(tvsFitted.getEstimatorParamMaps()))\n",
    "print(len(tvsFitted.validationMetrics))\n",
    "print(tvsFitted.validationMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6892163745019179, 0.510502314570867, 0.3664536939110651, 0.31679257987553894, 0.24655236616423126, 0.18633891607378847, 0.12416752564202191, 0.08844790966404317, 0.0634782659653297, 0.03650144877617272, 0.01753512868955874, 0.008457676545887327, 0.004137592979524203, 0.002055533225166825, 0.0010322038166535655, 0.0007264889100313303, 0.00033311393984353466, 0.0002064664704583792, 7.880561922556143e-05, 3.8462902620327634e-05, 2.1691150840552562e-05, 1.4897791984951597e-05, 7.593582642831163e-06, 4.03270219201795e-06, 2.031491353222154e-06, 1.0324961219027968e-06, 5.207674153398636e-07, 2.6302611265871177e-07, 1.3270426678462717e-07, 6.697408502800347e-08, 3.37985945598921e-08] 31\n",
      "+-------------------+------------------+\n",
      "|                FPR|               TPR|\n",
      "+-------------------+------------------+\n",
      "|                0.0|               0.0|\n",
      "|                0.0|0.7222222222222222|\n",
      "|                0.0|0.8611111111111112|\n",
      "|                0.0|               1.0|\n",
      "|0.20930232558139536|               1.0|\n",
      "| 0.4418604651162791|               1.0|\n",
      "|  0.627906976744186|               1.0|\n",
      "|  0.813953488372093|               1.0|\n",
      "|                1.0|               1.0|\n",
      "|                1.0|               1.0|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the est logistic model summary\n",
    "# since the model is pipeline\n",
    "# need to first get the classifier via stage\n",
    "# then get the summary \n",
    "summary = tvsFitted.bestModel.stages[1].summary\n",
    "loss_history = summary.objectiveHistory\n",
    "print(loss_history, len(loss_history))\n",
    "summary.roc.show()  # get the roc curve detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
